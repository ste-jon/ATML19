{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3967"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cover_dir = 'data/covers'\n",
    "cover_paths = os.listdir(cover_dir)\n",
    "len(cover_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000714685X_Religion & Spirituality.jpg', '002346450X_Politics & Social Sciences.jpg', '006095289X_Health, Fitness & Dieting.jpg', '006167012X_Religion & Spirituality.jpg', '006176678X_Biographies & Memoirs.jpg', '006198583X_Health, Fitness & Dieting.jpg', '006250407X_Christian Books & Bibles.jpg', '007166470X_Business & Money.jpg', '007786171X_Business & Money.jpg', '007877800X_Science & Math.jpg']\n"
     ]
    }
   ],
   "source": [
    "cover_paths.sort()\n",
    "print(cover_paths[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from utils_labels import category_string_to_number\n",
    "\n",
    "def load_images(data_root, image_paths, target_size):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for img_path in image_paths:\n",
    "        path = os.path.join(data_root, img_path)\n",
    "        img = Image.open(path)\n",
    "        img = img.resize(target_size)\n",
    "        images.append(np.array(img))\n",
    "        img_label = img_path[img_path.find(\"_\")+1:-4]\n",
    "        img_label = category_string_to_number(img_label)\n",
    "        labels.append(img_label)\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3967"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_imgs = 4000\n",
    "target_size = (227, 227)\n",
    "\n",
    "# we can't load many images this way\n",
    "cover_images, cover_labels = load_images(cover_dir, cover_paths[:n_imgs], target_size)\n",
    "len(cover_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3888\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# shuffle\n",
    "np.random.shuffle(cover_images)\n",
    "\n",
    "# set aside 20% for testing\n",
    "n_test = math.floor(len(cover_images) * 0.02)\n",
    "covers_train = cover_images[n_test:]\n",
    "labels_train =  cover_labels[n_test:]\n",
    "covers_test = cover_images[:n_test]\n",
    "labels_test =  cover_labels[:n_test]\n",
    "print(len(labels_train))\n",
    "print(labels_train[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset modifications "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount in the training set: 3888\n",
      "Amount in the validation set: 79\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import ToTensor, Normalize, Compose, Resize\n",
    "transform = Compose([   #Resize(224,224),\n",
    "                        ToTensor(),\n",
    "                        Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "                    ])\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CoverDataset(Dataset):\n",
    "    def __init__(self, data, target, transform):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.transform=transform\n",
    "    def __getitem__(self, index):\n",
    "        image = transform(self.data[index])\n",
    "        target = torch.tensor(self.target[index]).long() # needs floa\n",
    "        return image, target\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "# define the datasets\n",
    "train_dataset = CoverDataset(covers_train, labels_train, transform)\n",
    "test_dataset = CoverDataset(covers_test, labels_test, transform)\n",
    "\n",
    "\n",
    "print(\"Amount in the training set: \" + str(len(train_dataset)))\n",
    "print(\"Amount in the validation set: \" + str(len(test_dataset)))\n",
    "\n",
    "\n",
    "# define the dataloaders, batch_size 128\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "#val_loader = DataLoader(val_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PYTORCH: IMPORTING PRETRAINED ALEXNET\n",
    "\n",
    "##### Please note that \n",
    "##### i) The download may take a while\n",
    "##### ii) The pretrained models from the pytorch models have very specific requirements regarding input, see https://pytorch.org/docs/stable/torchvision/models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import *\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "alexnet = models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just as in the paper [], only retrain the last (fc) layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "  (1): ReLU(inplace)\n",
      "  (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (4): ReLU(inplace)\n",
      "  (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (7): ReLU(inplace)\n",
      "  (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (9): ReLU(inplace)\n",
      "  (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(inplace)\n",
      "  (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ") AdaptiveAvgPool2d(output_size=(6, 6)) Sequential(\n",
      "  (0): Dropout(p=0.5)\n",
      "  (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "  (2): ReLU(inplace)\n",
      "  (3): Dropout(p=0.5)\n",
      "  (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (5): ReLU(inplace)\n",
      "  (6): Linear(in_features=4096, out_features=30, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for param in alexnet.parameters():\n",
    "    param.requires_grad = False # == do not change weights, do not re-train\n",
    "\n",
    "# fixed, pre-trained alexnet. Now, replace the last layer:\n",
    "alexnet.classifier._modules['6'] = nn.Linear(4096, 30)\n",
    "print(*list(alexnet.children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: train_loss: 3.3801, train_accuracy: 21.2449, val_loss: 3.3710, val_accuracy: 16.4557\n",
      "Epoch 2/20: train_loss: 2.0797, train_accuracy: 34.0278, val_loss: 3.4963, val_accuracy: 21.5190\n",
      "Epoch 3/20: train_loss: 1.7054, train_accuracy: 43.3385, val_loss: 4.0301, val_accuracy: 11.3924\n",
      "Epoch 4/20: train_loss: nan, train_accuracy: 46.6307, val_loss: 3.9739, val_accuracy: 13.9241\n",
      "Epoch 5/20: train_loss: nan, train_accuracy: 49.5885, val_loss: 4.0978, val_accuracy: 15.1899\n",
      "Epoch 6/20: train_loss: 1.4223, train_accuracy: 52.1091, val_loss: 4.2184, val_accuracy: 22.7848\n",
      "Epoch 7/20: train_loss: 1.4340, train_accuracy: 52.8292, val_loss: 4.2763, val_accuracy: 12.6582\n",
      "Epoch 8/20: train_loss: nan, train_accuracy: 56.4815, val_loss: 4.6584, val_accuracy: 11.3924\n",
      "Epoch 9/20: train_loss: nan, train_accuracy: 56.8673, val_loss: 4.8274, val_accuracy: 18.9873\n",
      "Epoch 10/20: train_loss: 1.3638, train_accuracy: 57.5874, val_loss: 4.8384, val_accuracy: 15.1899\n",
      "Epoch 11/20: train_loss: 1.3022, train_accuracy: 59.1049, val_loss: 4.9915, val_accuracy: 17.7215\n",
      "Epoch 12/20: train_loss: 1.3350, train_accuracy: 57.5617, val_loss: 5.1817, val_accuracy: 11.3924\n",
      "Epoch 13/20: train_loss: 1.2749, train_accuracy: 59.4136, val_loss: 5.4070, val_accuracy: 12.6582\n",
      "Epoch 14/20: train_loss: nan, train_accuracy: 59.7994, val_loss: 5.5464, val_accuracy: 16.4557\n",
      "Epoch 15/20: train_loss: 1.2661, train_accuracy: 60.4938, val_loss: 5.2890, val_accuracy: 15.1899\n",
      "Epoch 16/20: train_loss: nan, train_accuracy: 60.5967, val_loss: 5.2982, val_accuracy: 16.4557\n",
      "Epoch 17/20: train_loss: 1.2815, train_accuracy: 60.2623, val_loss: 5.7876, val_accuracy: 17.7215\n",
      "Epoch 18/20: train_loss: nan, train_accuracy: 61.0082, val_loss: 5.5702, val_accuracy: 17.7215\n",
      "Epoch 19/20: train_loss: nan, train_accuracy: 60.4424, val_loss: 5.9395, val_accuracy: 17.7215\n",
      "Epoch 20/20: train_loss: 1.3110, train_accuracy: 59.2593, val_loss: 6.0077, val_accuracy: 16.4557\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(alexnet.parameters(), lr=0.003)\n",
    "\n",
    "from utils_train import train, test, fit\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "alexnet = alexnet.to(device)\n",
    "\n",
    "n_epochs = 20\n",
    "alexnet_retrain = fit(train_loader, test_loader, model=alexnet, optimizer=optimizer, loss_fn=loss_fn, n_epochs=n_epochs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
