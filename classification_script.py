import math
import numpy as np
import os
from torchvision.datasets import ImageFolder
from torchvision.transforms import ToTensor, Normalize, Compose, Resize
from torch.utils.data import DataLoader
from utils_labels import idx_to_class
from utils_labels import folder_to_cat_dict
import matplotlib.pyplot as plt


# import the covers
cover_dir_train = 'data/covers/train'
cover_dir_test = 'data/covers/test'
cover_dir_val = 'data/covers/valid'

# Normalization here given by pytorch (pretrained Alexnet -- Densenet?)
target_size = (224, 224)
transforms = Compose([Resize(target_size),
                    ToTensor(),
                    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
                    ])

train_dataset = ImageFolder(cover_dir_train, transform=transforms)
test_dataset = ImageFolder(cover_dir_test, transform=transforms)
val_dataset = ImageFolder(cover_dir_val, transform=transforms)

# class is the folder name and idx a number generated by ImageFolder
class_to_idx = train_dataset.class_to_idx
# mapping of folder to category name
folder_cat_path = 'data/meta/folder_to_cat.json'
folder_to_category = folder_to_cat_dict(folder_cat_path)

print("Amount in the training set: " + str(len(train_dataset)))
print("Amount in the test set: " + str(len(test_dataset)))
print("Amount in the validation set: " + str(len(test_dataset)))

# define the dataloaders, batch_size 128, 64, 32? Needs to be adjusted for the cluster
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)


# torch preparation
import torch
from torchvision import *
import torchvision.models as models
import torch.nn as nn
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
import torch.nn as nn
from utils_train import train, test, fit



### 
# 1) Import a pretrained Alexnet model from pytroch
# 2) Fix the weights for each layers
# 3) replace the last output layer with our custom one
###

num_classes = 32 # might change to 32?

alexnet = models.alexnet(pretrained=True)


for param in alexnet.parameters():
    param.requires_grad = False # == do not change weights, do not re-train


## fixed, pre-trained alexnet. Now, replace the last layer:
alexnet.classifier._modules['6'] = nn.Linear(4096, num_classes)
print(*list(alexnet.children()))  # show the model (optional)


# needs to be defined on the cluster training procedure
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(alexnet.parameters(), lr=0.003)


alexnet = alexnet.to(device)
n_epochs = 1
# retrain (only that last replaced layer)
alexnet_retrain = fit(train_loader, val_loader, model=alexnet, optimizer=optimizer, loss_fn=loss_fn, n_epochs=n_epochs)

# Print per lable accuracy
accuracy_per_label, accuracy_per_label_top3 = alexnet_retrain[4:6]
for it, nbr in enumerate(accuracy_per_label):
        print('{}: accuracy: {:.4f}, top 3 accuracy: {:.4f}'.format(folder_to_category[idx_to_class(it, class_to_idx)], accuracy_per_label[it], accuracy_per_label_top3[it]))
exit()
### 
# same procedure with "densenet161" (good performance on ImageNet, new concept)
###

densenet = models.densenet161(pretrained=True)

# fix the weights
for param in densenet.parameters():
    param.requires_grad = False

# replace the last output layer
densenet.classifier =  nn.Linear(2208, num_classes)
print(*list(densenet.children()))

# retrain densenet
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(densenet.parameters(), lr=0.003)
densenet = densenet.to(device)

n_epochs = 1
densenet_retrain = fit(train_loader, val_loader, densenet, optimizer, loss_fn, n_epochs)
